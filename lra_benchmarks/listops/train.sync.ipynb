{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c0e9e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T10:02:27.258077Z",
     "start_time": "2022-11-20T10:02:27.243259Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91194fa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T10:02:29.533870Z",
     "start_time": "2022-11-20T10:02:27.259648Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Copyright 2021 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Main training script for the listops task.\"\"\"\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from flax import jax_utils\n",
    "import flax.linen as nn\n",
    "from flax.metrics import tensorboard\n",
    "from flax.training import checkpoints\n",
    "from flax.training import common_utils\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.nn\n",
    "import jax.numpy as jnp\n",
    "from ml_collections import config_flags\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from configs.linformer_base import get_config\n",
    "from lra_benchmarks.listops import input_pipeline\n",
    "from lra_benchmarks.models.linformer import linformer\n",
    "\n",
    "# from lra_benchmarks.utils import train_utils\n",
    "from flax.training import checkpoints, train_state\n",
    "import optax\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e865dfd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T10:02:29.571189Z",
     "start_time": "2022-11-20T10:02:29.536783Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_weighted_cross_entropy(logits, targets, num_classes, weights=None):\n",
    "    \"\"\"Compute weighted cross entropy and entropy for log probs and targets.\n",
    "\n",
    "    Args:\n",
    "     logits: [batch, num_classes] float array.\n",
    "     targets: categorical targets [batch, length] int array.\n",
    "     num_classes: int, num classes of problem.\n",
    "     weights: None or array of shape [batch x length]\n",
    "\n",
    "    Returns:\n",
    "      Tuple of scalar loss and batch normalizing factor.\n",
    "    \"\"\"\n",
    "    onehot_targets = common_utils.onehot(targets, num_classes)\n",
    "    loss = -jnp.sum(onehot_targets * nn.log_softmax(logits), axis=-1)\n",
    "    normalizing_factor = onehot_targets.sum()\n",
    "    if weights is not None:\n",
    "        loss = loss * weights\n",
    "        normalizing_factor = weights.sum()\n",
    "\n",
    "    return loss.sum(), normalizing_factor\n",
    "\n",
    "\n",
    "def compute_weighted_accuracy(logits, targets, weights=None):\n",
    "    \"\"\"Compute weighted accuracy for log probs and targets.\n",
    "\n",
    "    Args:\n",
    "     logits: [batch, num_classes] float array.\n",
    "     targets: categorical targets [batch] int array.\n",
    "     weights: None or array of shape [batch]\n",
    "\n",
    "    Returns:\n",
    "      Tuple of scalar accuracy and batch normalizing factor.\n",
    "    \"\"\"\n",
    "    if logits.ndim != targets.ndim + 1:\n",
    "        raise ValueError(\n",
    "            \"Incorrect shapes. Got shape %s logits and %s targets\"\n",
    "            % (str(logits.shape), str(targets.shape))\n",
    "        )\n",
    "    loss = jnp.equal(jnp.argmax(logits, axis=-1), targets)\n",
    "    normalizing_factor = np.prod(logits.shape[:-1])\n",
    "    if weights is not None:\n",
    "        loss = loss * weights\n",
    "        normalizing_factor = weights.sum()\n",
    "\n",
    "    return loss.sum(), normalizing_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4508923",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T10:02:34.182337Z",
     "start_time": "2022-11-20T10:02:29.572964Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# if len(argv) > 1:\n",
    "#     raise app.UsageError(\"Too many command-line arguments.\")\n",
    "\n",
    "# tf.enable_v2_behavior()\n",
    "\n",
    "\n",
    "config = get_config()\n",
    "logging.info(\"===========Config Dict============\")\n",
    "logging.info(config)\n",
    "batch_size = config.batch_size\n",
    "learning_rate = config.learning_rate\n",
    "num_train_steps = config.num_train_steps\n",
    "num_eval_steps = config.num_eval_steps\n",
    "eval_freq = config.eval_frequency\n",
    "random_seed = config.random_seed\n",
    "model_type = config.model_type\n",
    "model_kwargs = config.model_kwargs.to_dict() if \"model_kwargs\" in config else {}\n",
    "\n",
    "# if jax.process_index() == 0:\n",
    "#     summary_writer = tensorboard.SummaryWriter(\n",
    "#         os.path.join(FLAGS.model_dir, \"summary\")\n",
    "#     )\n",
    "\n",
    "if batch_size % jax.device_count() > 0:\n",
    "    raise ValueError(\"Batch size must be divisible by the number of devices\")\n",
    "\n",
    "train_ds, eval_ds, test_ds, encoder = input_pipeline.get_datasets(\n",
    "    n_devices=jax.local_device_count(),\n",
    "    task_name=\"basic\",\n",
    "    data_dir=\"/data/lra_data/listops-1000/\",\n",
    "    batch_size=batch_size,\n",
    "    max_length=config.max_length,\n",
    ")\n",
    "\n",
    "vocab_size = encoder.vocab_size\n",
    "train_ds = train_ds.repeat()\n",
    "train_iter = iter(train_ds)\n",
    "max_length = config.max_length\n",
    "input_shape = (batch_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939fff22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T10:02:35.371335Z",
     "start_time": "2022-11-20T10:02:34.185652Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_kwargs.update(\n",
    "    {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"emb_dim\": config.emb_dim,\n",
    "        \"num_heads\": config.num_heads,\n",
    "        \"num_layers\": config.num_layers,\n",
    "        \"qkv_dim\": config.qkv_dim,\n",
    "        \"mlp_dim\": config.mlp_dim,\n",
    "        \"max_len\": config.max_length,\n",
    "        \"classifier\": True,\n",
    "        \"num_classes\": 10,\n",
    "    }\n",
    ")\n",
    "if \"model\" in config:\n",
    "    model_kwargs.update(config.model)\n",
    "\n",
    "rng = random.PRNGKey(random_seed)\n",
    "rng = jax.random.fold_in(rng, jax.process_index())\n",
    "rng, init_rng = random.split(rng)\n",
    "# We init the first set of dropout PRNG keys, but update it afterwards inside\n",
    "# the main pmap'd training update for performance.\n",
    "rng, dropout_rng = random.split(rng, jax.local_device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430f2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T08:51:08.248047Z",
     "start_time": "2022-11-20T08:51:01.169477Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d86b32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T10:02:35.419618Z",
     "start_time": "2022-11-20T10:02:35.375190Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loss_fn(state, params, inputs, targets, rngs):\n",
    "    \"\"\"Loss function used for training.\"\"\"\n",
    "    logits = state.apply_fn(params, inputs, rngs=rngs, train=True)\n",
    "    loss, weight_sum = compute_weighted_cross_entropy(\n",
    "        logits, targets, num_classes=10, weights=None\n",
    "    )\n",
    "    mean_loss = loss / weight_sum\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch, rngs):\n",
    "    \"\"\"Perform a single training step.\"\"\"\n",
    "    train_keys = [\"inputs\", \"targets\"]\n",
    "    (inputs, targets) = [batch.get(k, None) for k in train_keys]\n",
    "    grad_fn = jax.value_and_grad(loss_fn, argnums=1)\n",
    "    loss, grad = grad_fn(state, state.params, inputs, targets, rngs)\n",
    "    # new_state = state.apply_gradients(grads=grad)\n",
    "    return loss, grad\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(state, grad):\n",
    "    return state.apply_gradients(grads=grad)\n",
    "\n",
    "\n",
    "def rng_split(rngs):\n",
    "    splitted_rngs = jax.random.split(rngs[\"params\"], len(rngs))\n",
    "    apply_rng = dict(zip(rngs.keys(), splitted_rngs))\n",
    "    return apply_rng\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(batch, state):\n",
    "    train_keys = [\"inputs\", \"targets\"]\n",
    "    (inputs, targets) = [batch.get(k, None) for k in train_keys]\n",
    "    logits = state.apply_fn(state.params, inputs, rngs=rngs, train=False)\n",
    "    acc, _ = compute_weighted_accuracy(logits, targets, None)\n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17969c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T10:02:43.818392Z",
     "start_time": "2022-11-20T10:02:35.422522Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# inputs = jnp.ones(input_shape)\n",
    "\n",
    "batch = next(train_iter)\n",
    "batch = jax.tree_map(lambda x: x._numpy(), batch)\n",
    "inputs=batch['inputs']\n",
    "model = linformer.LinformerEncoder()\n",
    "rngs = {\"params\": init_rng, \"dropout\": dropout_rng}\n",
    "params = model.init(rngs, inputs, **model_kwargs)\n",
    "\n",
    "WARM_UP = 1000\n",
    "INIT_LR = 1e-4\n",
    "TOTAL_STEP = num_train_steps\n",
    "scheduler = optax.join_schedules(\n",
    "    [\n",
    "        optax.linear_schedule(0, INIT_LR, WARM_UP),\n",
    "        optax.cosine_decay_schedule(INIT_LR, TOTAL_STEP - WARM_UP),\n",
    "    ],\n",
    "    [WARM_UP],\n",
    ")\n",
    "tx = optax.adamw(scheduler)\n",
    "# learning_rate_fn = create_learning_rate_scheduler(base_learning_rate=learning_rate)\n",
    "# tx = optax.adamw(learning_rate, b1=0.9, b2=0.98, eps=1e-9, weight_decay=0.1)\n",
    "# learning_rate_fn(6000)\n",
    "\n",
    "init_state = train_state.TrainState.create(\n",
    "    apply_fn=functools.partial(model.apply, **model_kwargs), params=params, tx=tx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6272d365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-20T10:17:46.832647Z",
     "start_time": "2022-11-20T10:10:44.147126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6569516ccc3d423cba63f497e9deaf0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:1.9615116119384766,accuracy:0.3655754327774048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state = init_state\n",
    "for step in tqdm(range(0, num_train_steps)):\n",
    "    batch = next(train_iter)\n",
    "    # batch = common_utils.shard(\n",
    "    batch = jax.tree_map(lambda x: x._numpy(), batch)\n",
    "    # )  # pylint: disable=protected-access\n",
    "\n",
    "    loss, grad = train_step(state, batch, rngs)\n",
    "    state = update(state, grad)\n",
    "    rngs = rng_split(rngs)\n",
    "\n",
    "    # if step % 10 == 0:\n",
    "    #     logits=model.apply(state.params, inputs[:2], rngs=rngs, **model_kwargs, train=False)\n",
    "    #     print(logits)\n",
    "\n",
    "acc = 0\n",
    "for n,batch in enumerate(eval_ds):\n",
    "    batch = jax.tree_map(lambda x: x._numpy(), batch)\n",
    "    metrics = eval_step(batch, state)\n",
    "    acc += metrics[\"accuracy\"] / 32\n",
    "acc=acc/(n+1)\n",
    "# acc, _ = compute_weighted_accuracy(logits, targets, None)\n",
    "print(f\"loss:{loss},accuracy:{acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
